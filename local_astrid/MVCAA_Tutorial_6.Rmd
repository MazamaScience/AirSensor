---
title: "MVCAA Tutorial 6: Methow Vallely Smoke"
author: "Mazama Science"
date: "2021-03-30"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MVCAA Tutorial 5: Building a Local Archive}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(fig.width = 7, fig.height = 5)
```

## Introduction
This tutorial demonstrates how to assess the functioning of PA sensors by applying 
some of the AirSensor functions explored in the previous tutorials of this series.
In order to run the code in this tutorial you must have 
followed the instructions in Tutorial 5 and created a local multi-month archive 
of PurpleAir timeseries data. Target audiences include grad students, 
researchers and any member of the public concerned about air quality and 
comfortable working with R and RStudio.

Tutorials in this series include:

* [MVCAA Tutorial 1: Creating PAT Data](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_1.html)
* [MVCAA Tutorial 2: Exploring PAT Data](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_2.html)
* [MVCAA Tutorial 3: Creating Airsensor Data](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_3.html)
* [MVCAA Tutorial 4: Exploring Airsensor Data](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_4.html)
* [MVCAA Tutorial 5: Building a Local Archive](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_5.html)
* [MVCAA Tutorial 6: Methow Vallely Smoke](https://mazamascience.github.io/AirSensor/articles/articles/MVCAA_Tutorial_6.html)

## Goal
The overarching goal is to show how to evaluate if PA sensors functioned well
under extreme environmental conditions such us the high level of PM25 concentrations
in the air caused by the wildfires that scorched the Pacific Northwest and California during 
Fall 2020. We are going to explore the data and then compare measurements between 
PA sensors and the closest FRM monitor in the Methow Valley.

## Intro
On September 6 and 7 two wildfires were ignited not far from the Methow Valley – 
Cold Springs and Pearl Hill, which burned over 410,000 acres throughout the month.
Additionally, at the beginning of September several wildfires exploded in the 
southern, central and northern Oregon, and a “super massive” plume was carried 
to Washington State by the southerly winds. Eventually the smoke settled in the 
valleys and basins of Washington and air pollution reached unhealthy levels 
across the state (https://wasmoke.blogspot.com/). September was definitely the 
smokiest month during the Fall season as shown in the timeseries plots below.
 

## Data and Graphics 

Let's begin our journey by plotting and comparing the hourly values with daily 
averages for all sensors and all monitors in the Methow Valley during September 2020.

```{r directory-setup, eval = TRUE, warning = FALSE, message = FALSE}
# libraries
library(AirSensor)
library(PWFSLSmoke)
library(AirMonitorPlots)
library(ggplot2)

# Use the tutorial default archiveDir unless it is already defined
if ( !exists("archiveDir") ) {
  archiveDir <- file.path("C:/Users/astri/Mirror/Mazamascience/Projects/AirSensor/Data2/MVCAA/")}

setArchiveBaseDir(archiveDir)

# Set your archive base directory and check that is correct
setArchiveBaseDir(archiveDir)
getArchiveBaseDir()

```

```{r fall-overview-sensors, eval = TRUE, warning = FALSE, message = FALSE}
# load September sensors' data 
all_sensors_fall <- 
  sensor_load(
    collection = "mvcaa",  
    startdate = 20200901,
    enddate = 20201201,
    timezone = "America/Los_Angeles"
  )

# Plot all hourly values with daily averages
all_sensors_fall %>%
  AirMonitorPlots::ggplot_pm25Timeseries() + 
  ggplot2::ggtitle("Methow Valley PA Sensors -- Fall, 2020") +
  AirMonitorPlots::geom_pm25Points(shape = "square", alpha = .1) + 
  AirMonitorPlots::stat_dailyAQCategory(alpha = .5) + 
  ggplot2::scale_y_continuous(limits = c(0, 300)) +
  AirMonitorPlots::custom_aqiStackedBar(width = 0.01) 
```

```{r r fall-overview-monitors, eval = TRUE, warning = FALSE, message = FALSE}
# define longitude and latitude 
xlim <- c(-120.093, -120.284)
ylim <- c(48.313, 48.510)

# load September monitors' data 
mv_monitors_fall <- PWFSLSmoke::monitor_load(startdate = 20200901, enddate = 20201201) %>%
  monitor_subset(xlim = xlim, ylim = ylim)

# Plot all hourly values with daily averages
mv_monitors_fall %>%
  AirMonitorPlots::ggplot_pm25Timeseries() + 
  ggplot2::ggtitle("Methow Valley FRM Monitors -- Fall, 2020") +
  AirMonitorPlots::geom_pm25Points(shape = "square", alpha = .1) + 
  AirMonitorPlots::stat_dailyAQCategory(alpha = .5) + 
  ggplot2::scale_y_continuous(limits = c(0, 300)) +
  AirMonitorPlots::custom_aqiStackedBar(width = 0.01)
```
Data from the FRM monitors and the PA sensors have similar trends.
Let's see more closely how PA sensors performed compared to the Twisp monitor
during September 2020.

```{r prep-monitor-data, eval = TRUE, warning = FALSE, message = FALSE}

# find federal monitors in the Methow Valley

# define longitude and latitude 
xlim <- c(-120.093, -120.284)
ylim <- c(48.313, 48.510)

# load Sep data
mv_monitors <- PWFSLSmoke::monitor_load(startdate = 20200901, enddate = 20201001) %>%
  monitor_subset(xlim = xlim, ylim = ylim)

# select most useful meta data
names(mv_monitors$meta)
mv_monitors_meta <- as_tibble(mv_monitors$meta) %>%
  select(monitorID, siteName, longitude, latitude)

print(mv_monitors_meta$siteName) #"Winthrop-Chewuch Rd" "Twisp-Glover St" 


```

```{r prep-sensor-data, eval = TRUE, warning = FALSE, message = FALSE}
# load mvcaa pas file 
mvcaa <- get(load(file.path(archiveDir, "mvcaa.rda")))

# find sensors close to the monitor "Twisp-Glover St"
sensors <- pas_filterNear(pas = mvcaa, longitude = -120.1211, latitude = 48.3645, 
                          radius = "10 km")

# select most useful meta data
names(sensors)
sensors_meta <- as_tibble(sensors) %>%
  select(label, deviceDeploymentID, latitude, longitude)

# create sensor IDs object (must be as.character())
sensorIDs <- as.character(unique(sensors_meta$deviceDeploymentID))
print(sensorIDs) # 3 sensors 

# subset airsensor file for September using sensorIDs
sensors_subset <- 
  sensor_load(
    collection = "mvcaa",  
    startdate = 20200901,
    enddate = 20201001,
    timezone = "America/Los_Angeles") %>%
  monitor_subset(monitorIDs = sensorIDs)

# check if all 4 sensors are present
print(sensors_subset$meta$deviceDeploymentID)
# if we didn't have data for a sensor during September, 
# that sensor would not be present although it would appear in the $meta.
```

```{r dailyBarplot-Twisp-Glover, eval = TRUE, warning = FALSE, message = FALSE}
# create a dailyBarplot for the Twisp-Glover monitor
print(mv_monitors_meta) # find monitorID 

Twisp <- mv_monitors %>%
  monitor_subset(monitorIDs = "530470009_01")

monitor_dailyBarplot(
  Twisp,
  main = "Daily Average PM2.5 for Twisp -- Sep 2020",
  axes = TRUE)
addAQILegend(cex = 0.7)

```

```{r dailyBarplot-sensors, eval = TRUE, warning = FALSE, message = FALSE}
# create short name for dailyBarplot title 
sensors_subset$meta$shortName <-
  sensors_subset$meta$siteName %>%
  stringr::str_replace("MV Clean Air Ambassador @", "") %>%
  stringr::str_replace("MV Clean Air Ambassador-", "") %>%
  stringr::str_trim()

# create dailyBarplot for the 4 sensors 
layout(matrix(seq(4)))
opar <- par(mar = c(1,1,1,1))
on.exit(par(opar))
for (ID in sensorIDs) {
  siteName <- sensors_subset$meta[ID, 'shortName']
  monitor_dailyBarplot(
    sensors_subset,
    monitorID = ID,
    main = siteName,
    axes = TRUE
  )
  addAQILegend(cex = 0.7)
}
par(opar)
layout(1)
```
 
We can see that the Twisp monitor recorded increasing PM2.5
concentrations up to very unhealthy levels between ~ Sep 10 and 19 and that three out of
four sensors within a radius of 10 km from the monitor don't show data during the smokiest day.
The [QC algorithm AB_01](https://mazamascience.github.io/AirSensor/reference/PurpleAirQC_hourly_AB_01.html), that we used into the function `pat_createAirSensor()` in
Tutorial 5, is potentially invalidating many hourly values on those days, presumably
because the A and B PM2.5 values differ by more than is allowed by the algorithm.
We can demonstrate this and address the problem by creating a new sensor object
for each sensor by applying the [QC algorithm AB_00](https://mazamascience.github.io/AirSensor/reference/PurpleAirQC_hourly_AB_00.html)
and replot the data. But first, let's explore the raw data by running `pat_internalFit()`
for a quick assessment.
We'll then compare each sensor performance against the Twisp monitor by fitting
a linear model (monitor ~ sensor) with the `pat_externalFit()` function.

*NOTE*: By looking at the `$meta` of each `pat` objects, we can see that the current
nearest monitor is different from the monitor that collected PM2.5 concentrations
during September 2020. For this reason, we are going to edit the `$meta` of each `pat` object
by replacing the current `pwfsl_closestMonitorID` with the one active in September 2020.


```{r pat-exploration, eval = TRUE, warning = FALSE, message = FALSE}
#--------------- pat internal fits----------------------------------------------
Balky_Hill <- pat_loadMonth(
  id = "ab5dca99422f2c0d_13669",
  datestamp = 202009,
  timezone = "America/Los_Angeles"
)
pat_internalFit(Balky_Hill)

Benson_Creek <- pat_loadMonth(
  id = "f6c44edd41c941c7_10182",
  datestamp = 202009,
  timezone = "America/Los_Angeles"
)
  
pat_internalFit(Benson_Creek)

Liberty_Bell <- pat_loadMonth(
  id = "db5d6b3b79f5830e_39237",
  datestamp = 202009,
  timezone = "America/Los_Angeles"
)

pat_internalFit(Liberty_Bell)


Beaver_Creek <- pat_loadMonth(
  id = "2e3b5ceea86a885b_10168",
  datestamp = 202009,
  timezone = "America/Los_Angeles"
)

pat_internalFit(Beaver_Creek)

#--------------- pat external fits----------------------------------------------
# First, substitute the current closest monitor with the 2020 Twisp
# monitor ID. 

# check the current closest monitor 
Balky_Hill$meta$pwfsl_closestMonitorID #"840530470016_01"

# assign the Twisp monitor ID to pwfsl_closestMonitorID 
Balky_Hill$meta$pwfsl_closestMonitorID <- "530470009_01"
Benson_Creek$meta$pwfsl_closestMonitorID <- "530470009_01"
Liberty_Bell$meta$pwfsl_closestMonitorID <- "530470009_01"
Beaver_Creek$meta$pwfsl_closestMonitorID <- "530470009_01"

# Twisp ~ Balky_Hill
lm_BH <- pat_externalFit(
  pat = Balky_Hill,
  showPlot = TRUE
)

# Twisp ~ Benson_Creek
lm_BC <- pat_externalFit(
  pat = Benson_Creek,
  showPlot = TRUE
)

# Twisp ~ Liberty_Bell
lm_LB <- pat_externalFit(
  pat = Liberty_Bell,
  showPlot = TRUE
)
# distance: 4.2 km
# fit: 0.901 (missing days)

# Twisp ~ Beaver_Creek
lm_BeC <- pat_externalFit(
  pat = Beaver_Creek,
  showPlot = TRUE
)
```

The internal fit plots show that all sensors did quite a good job at measuring
the PM25 concentrations,
and that only the Benson Creek sensor had multiple days with missing data.
When compared to the Twisp monitor data, the external fit plots show that Balky 
Hill was the one performing best, followed by Liberty Bell High School, and Upper Beaver Creek
(we are not considering Benson Creek due to the substantial missing data during the
smokiest days). 
Given the thousands of observations included in
each data set, we would expect internal and external fits with an r-squared value of
~ 0.99 and a slope of ~ 1 for sensors that functioned very well.
However, when it comes to external fits, it is important to consider geographic 
features such as distance and topography. Indeed, topography affects air movement 
and therefore PM25 concentrations, having an effect on the external fit results. And indeed, 
shorter distance allows for similar measurements if topography in not interfering significantly.  
For instance, by looking at the map
below we notice that Balky Hill was the closest to Twisp, that there are mountains 
dividing Upper Beaver Creek and Twisp, and that Liberty Bell High School is the farthest
sensor but that the terrain between this and Twisp is flat.

 
```{r sensors-objects-AB00-and-map, eval = TRUE, warning = FALSE, message = FALSE}
#--- create sensor objects using AB_00 ----
BH_00 <- 
  Balky_Hill %>%
  pat_createAirSensor(
    FUN = PurpleAirQC_hourly_AB_00
  )

BC_00 <- 
  Benson_Creek %>%
  pat_createAirSensor(
    FUN = PurpleAirQC_hourly_AB_00
  )

LB_00 <- 
  Liberty_Bell %>%
  pat_createAirSensor(
    FUN = PurpleAirQC_hourly_AB_00
  )

BeC_00 <- 
  Beaver_Creek %>%
  pat_createAirSensor(
    FUN = PurpleAirQC_hourly_AB_00
  )

# combine sensors and monitor 
mapList <- list(BH_00, BC_00, LB_00, BeC_00, Twisp)
mapMonitors <- monitor_combine(mapList)

# create a leaflet map 
monitor_leaflet(mapMonitors)
```

Now that we have created the new sensor objects by applying the QC algorithm AB_00, 
we can replot the data. 

```{r dailyBarplot-sensors-AB00, eval = TRUE, warning = FALSE, message = FALSE}
# combine all sensors objects into a list of sensors 
sensorList <- list(BH_00, BC_00, LB_00, BeC_00)
sensors_subset_00 <- monitor_combine(sensorList)

# create short name for the plot titles 
sensors_subset_00$meta$shortName <-
  sensors_subset_00$meta$siteName %>%
  stringr::str_replace("MV Clean Air Ambassador @", "") %>%
  stringr::str_replace("MV Clean Air Ambassador-", "") %>%
  stringr::str_trim()


# create dailyBarplot for the 4 sensors 
layout(matrix(seq(4)))
opar <- par(mar = c(1,1,1,1))
on.exit(par(opar))
for (ID in sensorIDs) {
  siteName <- sensors_subset_00$meta[ID, 'shortName']
  monitor_dailyBarplot(
    sensors_subset_00,
    monitorID = ID,
    main = siteName,
    axes = TRUE
  )
  addAQILegend(cex = 0.7)
}
par(opar)
layout(1)
```
These plots confirm that, initially, the sensors' data was invalidated by the 
QC algorithm AB_01. By applying the QC algorithm AB_00, it was possible to 
obtain a more inclusive graphic, which better represent the data collected by the 
sensors.  

## Conclusion
PA sensors can be valuable tools to evaluate PM2.5 concentrations in the air but necessitate a close assessment to understand if they work properly. As we saw, the Airsensor package offers functions 
to monitor the ongoing and past state of heath of sensors, including variables such as temperature and humidity, and functions to evaluate, manipulate, visualize and map the data. 
In this tutorial, we wanted to show you how to quickly investigate the 
performance of multiple sensors, how to trouble shoot and fix potential issues, and how to interpret and visualize the data in effective ways. 
At the end of the day, it's your call to decide which algorithm to apply and which sensors to include in your study based on the overall sensor performance, 
including potential days with missing data, internal and external fits, and the 
geographic features surrounding the sensors.

_Best of luck assessing air quality in your community!_
 

